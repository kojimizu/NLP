---
title: "Text Mining with R"
author: "KM"
date: "2018/07/24"
output: 
 word: default
 pdf: default
 html: default 
---

Text Mining with R - A Tidy Approach
Julia Silge and David Robinson 
https://www.tidytextmining.com/


# Chapter 7: Case Study: Comparing Twitter Archives
One type of text that gets plenty of attention is text shared online via Twitter. In fact, several of the sentiment lexicons used in this book (and commonly used in general) were designed for use with and validated on tweets. Both of the authors of this book are on Twitter and are fairly regular users of it, so in this case study, let’s compare the entire Twitter archives of Julia and David.

## 7.1 Getting the data and distribution of tweets
An individual can download their own Twitter archive by following directions available on Twitter’s website. 
https://help.twitter.com/ja/managing-your-account/how-to-download-your-twitter-archive

We each downloaded ours and will now open them up. Let’s use the lubridate package to convert the string timestamps to date-time objects and initially take a look at our tweeting patterns overall (Figure 7.1).

julia and dave tweet data: https://github.com/kojimizu/tidy-text-mining/tree/master/data

```{r 7.1}

# package load
library(lubridate)
library(ggplot2)
library(dplyr)
library(readr)

tweets_julia <- read_csv("C:/Users/kojikm.mizumura/Desktop/Data Science/Text Mining with R/II. Case Study/Data/tweets_julia.csv")
tweets_dave <- read_csv("C:/Users/kojikm.mizumura/Desktop/Data Science/Text Mining with R/II. Case Study/Data/tweets_dave.csv")
head(tweets_dave)

tweets <- bind_rows(tweets_julia %>% 
                      mutate(person = "Julia"),
                    tweets_dave %>% 
                      mutate(person = "David")) %>%
  mutate(timestamp = ymd_hms(timestamp))

ggplot(tweets, aes(x = timestamp, fill = person)) +
  geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
  facet_wrap(~person, ncol = 1)
```

David and Julia tweet at about the same rate currently and joined Twitter about a year apart from each other, but there were about 5 years where David was not active on Twitter and Julia was. In total, Julia has about 4 times as many tweets as David.

## 7.2 Word frequencies
Let's use `unnest_tokens()` to make a tidy data frame of all the words in our tweets, and remove the common English stop words. There are certain conventions in how people use text on Twitter, so we will do a bit more owrk with our text here than, for example, we did with the narrative text from Project Gutenberg.

First, we will remove tweets from this dataset that are retweets so that we only have tweets that we wrote ourselves. Next, the `mutate()` line removes links and cleans our some characters that we don't want like ampersands and such. 

In the call to `unnest_tokens()`, we unnest using a regex pattern, instead of just looking for single unigrams (words). This regex pattern is very useful for dealing with Twitter text; it retains hashtags and mentions of usernames with the `@` symbol.

Because we have kept these types of symbols in the texzt, we can't use a simple `anti_join()` to remove stop words. Instead, we can take the approach shown in the `filter()` line that uses `str_detect()` from the stringr package. 
```{r 7.2-1}

library(tidytext)
library(stringr)

replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

tweets

# need to find out this formula for tidy data cleaning 
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

tidy_tweets %>% select(tweet_id, timestamp,person,word)
```

Now we can calculate word frequencies for each person. First, we group by person and count how many times each person used each word. Then we use `left_join()` to add a column of the total number of words used by each person. (This is higher for Julia than David since she has more tweets than David.) Finally, we calculate a frequency for each person and word.
```{r 7.2-2}
frequency <- tidy_tweets %>% 
  group_by(person) %>% 
  count(word,sort=TRUE) %>% 
  left_join(tidy_tweets %>% 
              group_by(person) %>% 
              summarise(total=n())) %>% 
  mutate(freq=n/total)

frequency
```

This is a nice and tidy data frame but we would actually like to plot those frequencies on the x- and y-axes of a plot, so we will need to use `spread()` from tidyr make a differently shaped data frame.
```{r 7.2-3}
library(tidyr)

frequency <- frequency %>% 
  select(person,word,freq) %>% 
  spread(person,freq) %>% 
  arrange(Julia,David) %>% 

frequency
```

Now this is ready for us to plot. Let’s use `geom_jitter()` so that we don’t see the discreteness at the low end of frequency as much, and `check_overlap = TRUE` so the text labels don’t all print out on top of each other (only some will print).
```{r 7.2-4}
library(scales)
library(ggplot2)

frequency
# ggplot(frequency, aes(Julia, David)) +
#   geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
#   geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
#   scale_x_log10(labels = percent_format()) +
#   scale_y_log10(labels = percent_format()) +
#   geom_abline(color = "red")

```

Words near the line in Figure 7.2 are used with about equal frequencies by David and Julia, while words far away from the line are used much more by one person compared to the other. Words, hashtags, and usernames that appear in this plot are ones that we have both used at least once in tweets.

This may not even need to be pointed out, but David and Julia have used their Twitter accounts rather differently over the course of the past several years. David has used his Twitter account almost exclusively for professional purposes since he became more active, while Julia used it for entirely personal purposes until late 2015 and still uses it more personally than David. We see these differences immediately in this plot exploring word frequencies, and they will continue to be obvious in the rest of this chapter.

## 7.3 Comparing word usage
We just made a plot comparing raw word frequencies over our whole Twitter histories; now let’s find which words are more or less likely to come from each person’s account using **the log odds** ratio. First, let’s restrict the analysis moving forward to tweets from David and Julia sent during 2016. David was consistently active on Twitter for all of 2016 and this was about when Julia transitioned into data science as a career.
```{r 7.3-1}
tidy_tweets <- tidy_tweets %>% 
  filter(timestamp >= as.Date("2016-01-01"),
         timestamp <=as.Date("2017-01-01"))

tidy_tweets%>% select(tweet_id, timestamp,person,word)
```

Next, let’s use `str_detect()` to remove Twitter usernames from the `word` column, because otherwise, the results here are dominated only by people who Julia or David know and the other does not. After removing these, we count how many times each person uses each word and keep only the words used more than 10 times. After a `spread()` operation, we can calculate the log odds ratio for each word, using
$$
log\,odds\,ratio = ln\frac{[\frac{n+1}{total+1}_{David}]}{[\frac{n+1}{total+1}_{Julia}]}
$$

where *n* is the number of times the word in question is used by each person and the total indicates the total words for each person.
```{r 7.3-2}
library(magrittr)
library(tidyverse)

word_ratios <- tidy_tweets %>% 
  filter(!str_detect(word,"^@")) %>% 
  count(word,person) %>% 
  filter(sum(n)>=10) %>% 
  ungroup() %>% 
  spread(person,n,fill=0) %>% 
  mutate_if(is.numeric,funs((.+1)/sum(.+1))) %>% 
  mutate(logratio=log(David/Julia)) %>% 
  arrange(desc(logratio))
```

What are some words that have been about equally likely to come from David or Julia’s account during 2016?
```{r 7.3-3}
word_ratios %>% 
  arrange(abs(logratio))
```

We are about equally likelt to tweer about maps, email, APIs, and functions. 

Which words are most likely to be from Julia's account or from David's account? Let's just take the top 15 most distinctive words fro each account and plot them in Figure 7.3.
```{r 7.3-4}
word_ratios %>% 
  group_by(logratio <0) %>% 
  top_n(15,abs(logratio)) %>% 
  ungroup() %>% 
  mutate(word=reorder(word,logratio)) %>% 
  ggplot(aes(word,logratio,fill=logratio<0))+
  geom_col(show.legend = F)+
  coord_flip()+
  ylab("log odds ratio (David/Julia")+
  scale_fill_discrete(name="",labels=c("David","Julia"))
```

So David has tweeted about specific conferences he has gone to, genes, Stack Overflow, and matrices while Julia tweeted about Utah, physics, Census data, Christmas, and her family.

## 7.4 Changes in word use
The section above looked at overall word use, but now let’s ask a different question. Which words’ frequencies have changed the fastest in our Twitter feeds? Or to state this another way, which words have we tweeted about at a higher or lower rate as time has passed? To do this, we will define a new time variable in the data frame that defines which unit of time each tweet was posted in. We can use `floor_date()` from lubridate to do this, with a unit of our choosing; using 1 month seems to work well for this year of tweets from both of us.

After we have the time bins defined, we count how many times each of us used each word in each time bin. After that, we add columns to the data frame for the total number of words used in each time bin by each person and the total number of times each word was used by each person. We can then `filter()` to only keep words used at least some minimum number of times (30, in this case).

```{r 7.4-1}
library(lubridate)

words_by_time <- tidy_tweets %>% 
  filter(!str_detect(word,"^@")) %>% 
  mutate(time_floor=floor_date(timestamp,unit="1 month")) %>% 
  count(time_floor,person,word) %>% 
  ungroup() %>% 
  group_by(person,time_floor) %>% 
  mutate(time_total=sum(n)) %>% 
  group_by(word) %>% 
  mutate(word_total=sum(n)) %>% 
  ungroup() %>% 
  rename(count=n) %>% 
  filter(word_total>30)

words_by_time
```

Each row in this data frame corresponds to one person using one word in a given time bin. The `count` column tells us how many times that person used that word in that time bin, the `time_total` column tells us how many words that person used during that time bin, and the `word_total` column tells us how many times that person used that word over the whole year. This is the data set we can use for modeling.

We can use `nest()` from tidyr to make a data frame with a list column that contains little miniature data frames for each word. Let's do that now nad take a look at the resulting structure.
```{r 7.4-2}
nested_data <- words_by_time %>% 
  nest(-word,-person)
nested_data

## # A tibble: 112 x 3
##    person word    data             
##    <chr>  <chr>   <list>           
##  1 David  #rstats <tibble [12 × 4]>
##  2 David  bad     <tibble [9 × 4]> 
##  3 David  bit     <tibble [10 × 4]>
##  4 David  blog    <tibble [12 × 4]>
##  5 David  broom   <tibble [10 × 4]>
##  6 David  call    <tibble [9 × 4]> 
##  7 David  check   <tibble [12 × 4]>
##  8 David  code    <tibble [10 × 4]>
##  9 David  data    <tibble [12 × 4]>
## 10 David  day     <tibble [8 × 4]> 
## # ... with 102 more rows
```

This data frame has one row for each person-word combination; the data column is a list column that contains data frames, one for each combination of person and word. Let’s use `map()` from the **purrr** library to apply our modeling procedure to each of those little data frames inside our big data frame. This is count data so let’s use `glm()` with `family = "binomial"` for modeling.
```{r}
library(purrr)

nested_models <- nested_data %>% 
  mutate(models=map(data,~glm(cbind(count,time_total)~
                                time_floor,.,family="binomial")))
nested_models
```

Now notice that we have a new column for the modeling results; it is another list column and contains `glm` objects. The next step is to use `map()` and `tidy()` from the **broom** package to pull out the slopes for each of these models and find the important ones. We are comparing many slopes here and some of them are not statistically significant, so let’s apply an adjustment to the p-values for multiple comparisons.
```{r 7.4-3}
library(broom)
library(tidytext)

slopes <- nested_models %>% 
  unnest(map(models,tidy)) %>% 
  filter(term=="time_floor") %>% 
  mutate(adjusted.p.value=p.adjust(p.value))
```

Now let’s find the most important slopes. Which words have changed in frequency at a moderately significant level in our tweets?
```{r 7.4-4}
slopes
top_slopes <- slopes %>% 
  filter(adjusted.p.value<0.1)
top_slopes
```

To visualize our results, we can plot these words’ use for both David and Julia over this year of tweets.
```{r}
words_by_time %>% 
  inner_join(top_slopes,by=c("word","person")) %>% 
  filter(person=="David") %>% 
  ggplot(aes(time_floor,count/time_total,color=word))+
  geom_line(size=1.3)+
  labs(x=NULL,y="Word frequency")
```

We see in Figure 7.4 that David tweeted a lot about the UseR conference while he was there and then quickly stopped. He has tweeted more about Stack Overflow toward the end of the year and less about ggplot2 as the year has progressed.

Now let’s plot words that have changed frequency in Julia’s tweets in Figure 7.5.
```{r}
words_by_time %>% 
  inner_join(top_slopes,by=c("word","person")) %>% 
  filter(person=="Julia") %>% 
  ggplot(aes(time_floor,count/time_total,color=word))+
  geom_line(size=1.3)+
  labs(x=NULL,y="Word frequency")
```

All the significant slopes for Julia are negative. This means she has not tweeted at a higher rate using any specific words, but instead using a variety of different words; her tweets earlier in the year contained the words shown in this plot at higher proportions. Words she uses when publicizing a new blog post like the #rstats hashtag and “post” have gone down in frequency, and she has tweeted less about reading.

## 7.5 Favorites and retweets 
Another important characteristic of tweets is how many times they are favorited or retweeted. Let’s explore which words are more likely to be retweeted or favorited for Julia’s and David’s tweets. When a user downloads their own Twitter archive, favorites and retweets are not included, so we constructed another dataset of the authors’ tweets that includes this information. We accessed our own tweets via the Twitter API and downloaded about 3200 tweets for each person. In both cases, that is about the last 18 months worth of Twitter activity. This corresponds to a period of increasing activity and increasing numbers of followers for both of us.
```{r 7.5-1}
library(tidyverse)
library(tidytext)
library(lubridate)

# data load
tweets_julia <- read_csv("C:/Users/kojikm.mizumura/Desktop/Data Science/Text Mining with R/II. Case Study/Data/juliasilge_tweets.csv")
tweets_dave <- read_csv("C:/Users/kojikm.mizumura/Desktop/Data Science/Text Mining with R/II. Case Study/Data/drob_tweets.csv")

# data merge
tweets <- bind_rows(tweets_julia %>% 
                      mutate(person="Julia"),
                    tweets_dave %>% 
                      mutate(person="David")) %>% 
  mutate(created_at=ymd_hms(created_at))
```

Now that we have this second, smaller set of only recent tweets, let’s use `unnest_tokens()` to transform these tweets to a tidy data set. Let’s remove all retweets and replies from this data set so we only look at regular tweets that David and Julia have posted directly.
```{r 7.5-2}
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^(RT|@)")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = unnest_reg) %>%
  anti_join(stop_words)

tidy_tweets
```

To start with, let’s look at the number of times each of our tweets was retweeted. Let’s find the total number of retweets for each person.
```{r 7.5-3}
tidy_tweets
totals <- tidy_tweets %>% 
  group_by(person,id) %>% 
  summarise(rts=sum(retweets)) %>% 
  group_by(person) %>% 
  summarise(total_rts=sum(rts))
totals
```

Now let’s find the median number of retweets for each word and person. We probably want to count each tweet/word combination only once, so we will use `group_by()` and `summarise()`   twice, one right after the other. The first `summarise()` statement counts how many times each word was retweeted, for each tweet and person. In the second `summarise()` statement, we can find the median retweets for each person and word, also count the number of times each word was used ever by each person and keep that in uses. Next, we can join this to the data frame of retweet totals. Let’s `filter()` to only keep words mentioned at least 5 times.
```{r 7.5.4}
library(magrittr)
library(tidyverse)

tidy_tweets
word_by_rts <- tidy_tweets %>% 
  group_by(id, word, person) %>% 
  summarise(rts = first(retweets)) %>% 
  group_by(person, word) %>% 
  summarise(retweets = median(rts), uses = n()) %>%
  left_join(totals) %>%
  filter(retweets != 0) %>%
  ungroup()

word_by_rts %>% 
  filter(uses >= 5) %>%
  arrange(desc(retweets))
```

At the top of this sorted data frame, we see tweets from Julia and David about packages that they work on, like gutenbergr, gganimate, and tidytext. Let’s plot the words that have the highest median retweets for each of our accounts (Figure 7.6)
```{r 7.5.5}
word_by_rts %>%
  filter(uses >= 5) %>%
  group_by(person) %>%
  top_n(10, retweets) %>%
  arrange(retweets) %>%
  ungroup() %>%
  mutate(word = factor(word, unique(word))) %>%
  ungroup() %>%
  ggplot(aes(word, retweets, fill = person)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ person, scales = "free", ncol = 2) +
  coord_flip() +
  labs(x = NULL, 
       y = "Median # of retweets for tweets containing each word")
```

We see lots of word about R packages, including tidytext, a package about which you are reading right now! The “0” for David comes from tweets where he mentions version numbers of packages, like “broom 0.4.0” or similar.

We can follow a similar procedure to see which words led to more favorites. Are they different than the words that lead to more retweets?
```{r 7.5.6}
totals <- tidy_tweets %>% 
  group_by(person, id) %>% 
  summarise(favs = sum(favorites)) %>% 
  group_by(person) %>% 
  summarise(total_favs = sum(favs))

word_by_favs <- tidy_tweets %>% 
  group_by(id, word, person) %>% 
  summarise(favs = first(favorites)) %>% 
  group_by(person, word) %>% 
  summarise(favorites = median(favs), uses = n()) %>%
  left_join(totals) %>%
  filter(favorites != 0) %>%
  ungroup()
```


We have bult the data frames we need. 
Now let's make our visualization in the following figure.
```{r 7.5.7}
word_by_favs %>%
  filter(uses >= 5) %>%
  group_by(person) %>%
  top_n(10, favorites) %>%
  arrange(favorites) %>%
  ungroup() %>%
  mutate(word = factor(word, unique(word))) %>%
  ungroup() %>%
  ggplot(aes(word, favorites, fill = person)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ person, scales = "free", ncol = 2) +
  coord_flip() +
  labs(x = NULL, 
       y = "Median # of favorites for tweets containing each word")
```

We see some minor differences between Figures 7.6 and 7.7, especially near the bottom of the top 10 list, but these are largely the same words as for retweets. In general, the same words that lead to retweets lead to favorites. A prominent word for Julia in both plots is the hashtag for the NASA Datanauts program that she has participated in; read on to Chapter 8 to learn more about NASA data and what we can learn from text analysis of NASA datasets.

## 7.6 Summary
This chapter was our first case study, a beginning-to-end analysis that demonstrates how to bring together the concepts and code we have been exploring in a cohesive way to understand a text data set. Comparing word frequencies allows us to see which words we tweeted more and less frequently, and the log odds ratio shows us which words are more likely to be tweeted from each of our accounts. We can use nest() and map() with the glm() function to find which words we have tweeted at higher and lower rates as time has passed. Finally, we can find which words in our tweets led to higher numbers of retweets and favorites. All of these are examples of approaches to measure how we use words in similar and different ways and how the characteristics of our tweets are changing or compare with each other. These are flexible approaches to text mining that can be applied to other types of text as well.


# Chaoter 8: Case Study mining NASA metadata
There are over 32,000 datasets hosted and/or maintained by NASA; these datasets cover topics from Earth science to aerospace engineering to management of NASA itself. We can use the metadata for these datasets to understand the connections between them.
https://www.nasa.gov/

The metadata includes information like the title of the dataset, a description filed, what organization(s) within NASA is responsible for the dataset, keywords for the dataset that have been assigned by a human being, and so forth.

NASA places a high priority on making its data open and accessible, even requiring all NASA-funded research to be openly accessible online. The metadata for all its datasets is publicly available online in JSON format.
https://www.nasa.gov/press-release/nasa-unveils-new-public-web-portal-for-research-results
https://data.nasa.gov/data.json

In this chapter, we will treat the NASA metadata as a text dataset and show how to implement several tidy text approaches with this real-life text. We will use word co-occurrences and correlations, tf-idf, and topic modeling to explore the connections between the datasets. Can we find datasets that are related to each other? Can we find clusters of similar datasets? Since we have several text fields in the NASA metadata, most importantly the title, description, and keyword fields, we can explore the connections between the fields to better understand the complex world of data at NASA. This type of approach can be extended to any domain that deals with text, so let’s take a look at this metadata and get started.


```{r}
txt_jp <- "羽鳥来日にあわせて猛者が次々に集結したという感じ"
txt_jp
quanteda::tokens(txt_jp)
```

## 8.1 How data is organized at NASA
First, let's download the JASON file and take a look at the names of what is stored in the metadata. 
```{r}
# install.packages("jsonlite")
library(jsonlite)

# Json file download does not work with fromJson() function
# metadata <- fromJSON("https://data.nasa.gov/data.jason")
# https://github.com/nasa/data.nasa.gov/tree/master/js

metadata <- fromJSON("C:/Users/kojikm.mizumura/Desktop/Data Science/Text Mining with R/data1.json")

names(metadata$dataset)
```


We see here that we could extract information from who publishes each dataset to what license they are released under.

It seems likely that the title, description, and keywords for each dataset may be most fruitful for drawing connections between datasets. Let’s check them out.
```{r}
class(metadata$dataset$title)
```


```{r}
class(metadata$dataset$description)
```

```{r}
class(metadata$dataset$keyword)
```

The title and description fields are stored as character vectors, but the keywords are stored as a list of character vectors.

### 8.1.1 Wrangling and tidying the data
Let's set up separate tidy data frames for title, description, and keyword, keeping the dataset ids fro each so that we can connect them later in the analaysis if necessary.
```{r}
library(dplyr)

nasa_title <- data_frame(id = metadata$dataset$`_id`$`$oid`, 
                         title = metadata$dataset$title)

nasa_title
```

These are just a few example titles from the dataset we will be exploring. Notice that we have the NASA-assigned ids here, and also that there are duplicate titles on separate datasets.
```{r}
nasa_desc <- data_frame(id = metadata$dataset$`_id`$`$oid`, 
                        desc = metadata$dataset$description)

nasa_desc %>% 
  select(desc) %>% 
  sample_n(5)
```

Here we see the first part of several selected description fields from the metadata.

Now we can build the tidy data frame for the keywords. For this one, we need to use `unnest()` from tidyr, because they are in a list-column.
```{r}
library(tidyr)

nasa_keyword <- data_frame(id = metadata$dataset$`_id`$`$oid`, 
                           keyword = metadata$dataset$keyword) %>%
  unnest(keyword)

nasa_keyword
```

This is a tidy data frame because we have one row each keyword; this means we will have multiple rows for each dataset bevcause a dataset can have more than one keyword.

Now it is a time to use tidytext's `unnest_tokens()` for the title and description fileds so we can do the text analysis. Let's also remove stop words from the titles and descriptions. We will not remove stop words from the keywords, because those are short, human-assigned keywords like "RADIATION" or "CLIMATE INDICATORS"

```{r}
library(tidytext)

nasa_title <- nasa_title %>% 
  unnest_tokens(word, title) %>% 
  anti_join(stop_words)

nasa_desc <- nasa_desc %>% 
  unnest_tokens(word, desc) %>% 
  anti_join(stop_words)
```

These are now in the tidy text format that we have been working with throughout this book, with one token (word, in this case) per row; let’s take a look before we move on in our analysis.
```{r}
nasa_title
```

```{r}
nasa_desc
```

### 8.1.2 Some initial simple exploration
What are the most common words in the NASA dataset titles?
We can use `count()` from dplyr to check this out. 

```{r}
nasa_title %>% 
  count(word,sort=T)
```

What about the descriptions?
```{r}
nasa_desc %>% 
  count(word,sort=TRUE)
```

Words like “data” and “global” are used very often in NASA titles and descriptions. We may want to remove digits and some “words” like “v1” from these data frames for many types of analyses; they are not too meaningful for most audiences.

# We can do this by making a list of custom stop words and using anti_join() to remove them from the data frame, just like we removed the default stop words that are in the tidytext package. This approach can be used in many instances and is a great tool to bear in mind. 

```{r}
my_stopwords <- data_frame(word=c(as.character(1:10),
                                  "v1","v03","l2","l3","v.5.2.0",
                                  "v003","v004","v005","v996","v7"))
nasa_title <- nasa_title %>% 
  anti_join(my_stopwords)
nasa_desc <- nasa_desc %>% 
  anti_join(my_stopwords)
```

What are the most common keywords?
```{r}
nasa_keyword %>% 
  group_by(keyword) %>% 
  count(sort=TRUE)
```

We likely want to change all of the keywords to either lower or uppoer case to get rid of duplicates like "OCEANS" and "Oceans". Let's do that here. 
```{r}
nasa_keyword <- nasa_keyword %>% 
  mutate(keyword=toupper(keyword))
```

## 8.2 Word co-occurences and correlations
As a next step, let's examine which words commonly occur together in the titles, descriptions, and keywords  of NASA datasets, as described in Chapter 4. We can the nexamine word networks of these fields; this may help us see, for example, which datasets are related to each other.

### 8.2.1 Networks of Description and title Words
We can use `pairwise_count()` from `windyr` package to count how many times each pair of words occurs together in a title or description field.
```{r include=FALSE}
library(widyr)
library(magrittr)
library(ggraph)
```


```{r}
nasa_title
title_word_pairs <- nasa_title %>% 
  pairwise_count(id,word, sort = TRUE, upper = FALSE)

title_word_pairs
```

These are the pairs of words that occur together most often in title fields. Some of these words are obviously acronyms used within NASA, and we see how often words like “project” and “system” are used.

```{r}
desc_word_pairs <- nasa_desc %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE)
nasa_desc
desc_word_pairs

```

These are the pairs of words that occur together most often in description fields. "Data" is a very common word in description fileds; there is no shortage of data in the datasets at NASA!

Let's plot networks of these co-occurring words so we can see these relationships better in Figure 8.1. We will again use the ggraph package for visualizing our networks.
```{r}
library(ggplot2)
library(igraph)
library(ggraph)

set.seed(1234)
title_word_pairs %>%
  filter(n >= 250) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

We see some clear clustering in this network of title words;words in NASA dataset titles are largely organized into several families of words that tend to go together.

What about the words from the description fields?
```{r}
set.seed(1234)
desc_word_pairs %>% 
  filter(n>=5000) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout="fr")+
  geom_edge_link(aes(edge_alpha=n,edge_width=n),edge_colour="darkred")+
  geom_node_point(size=5)+
  geom_node_text(aes(label=name),repel=TRUE,
                 point.padding=unit(0.2,"lines"))+
  theme_void()
```

Figure 8.2 shows such *strong* connections between the top dozen or so words (words like “data”, “global”, “resolution”, and “instrument”) that we do not see clear clustering structure in the network. We may want to use tf-idf (as described in detail in Chapter 3) as a metric to find characteristic words for each description field, instead of looking at counts of words.

### 8.2.2 Networks of Keywords 
Next, let's make a network of the keywords in Figure 8.3 to see which keywords commonly occur together in the same datasets:

```{r}
keyword_pairs <- nasa_keyword %>% 
  pairwise_count(keyword, id, sort = TRUE, upper = FALSE)

keyword_pairs
```

```{r}
set.seed(1234)
keyword_pairs %>%
  filter(n >= 700) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "royalblue") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

We definitely see clustering here, and strong connections between keywords like “OCEANS”, “OCEAN OPTICS”, and “OCEAN COLOR”, or “PROJECT” and “COMPLETED”.

To examine the relationships among keywords in a different way, we can find the correlation among the keywords as described in Chapter 4. This looks for those keywords that are more likely to occur together than with other keywords in a description field.

```{r}
library(tidyverse)
library(magrittr)
library(tidytext)

keyword_cors <- nasa_keyword %>% 
  group_by(keyword) %>% 
  filter(n() >=50) %>% 
  pairwise_cor(keyword,id,sort=TRUE,upper=FALSE)
keyword_cors 
```

Notice that these keywords at the top of this sorted data frame have correlation coefficients equal to 1; they always occur together. This means these are redundant keywords. It may not make sense to continue to use both of the keywords in these sets of pairs; instead, just one keyword could be used.

Let’s visualize the network of keyword correlations, just as we did for keyword co-occurences.
```{r}
set.seed(1234)
ketword_cors %>% 
  filter(correlation>.6) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout="fr")+
  geom_edge_link(aes(edge_alpha=correlation,edge_width=correlation),edge_colour="royalblue")+
  geom_node_point(size=5)+
  geom_node_text(aes(label=name),repel=T,
                 point.padding=unit(0.2,"lines"))+
  theme_void()
```

This network in Figure 8.4 appears much different than the co-occurence network. The difference is that the co-occurrence network asks a question about which keyword pairs occur most often, and the correlation network asks a question about which keywordsoccur more often together than with other keywords. Notice here the high number of small clusters of keywords; the network structure can be extracted (for further analysis) from the `graph_from_data_frame()` function above.

## 8.3 Calculating tf-idf for the description fields 

The network graph in Figure 8.2 showed us that the description fields are dominated by a few common words like “data”, “global”, and “resolution”; this would be an excellent opportunity to use tf-idf as a statistic to find characteristic words for individual description fields. As discussed in Chapter 3, we can use tf-idf, the term frequency times inverse document frequency, to identify words that are especially important to a document within a collection of documents. Let’s apply that approach to the description fields of these NASA datasets.

### 8.3.1 What is tf-idf for the description field words?
We will consider each description field a document, and the whole set of description fields the collection or corpus of documents. We have already used `unnest_tokens()` earlier in this chapter to make a tidy data frame of the words in the description fields, so now we can use `bind_tf_idf()` to calculate tf-idf for each word.

```{r}
desc_tf_idf <- nasa_desc %>% 
  count(id,word,sort=T) %>% 
  ungroup() %>% 
  bind_tf_idf(word,id,n)
```

What are the highest tf-idf words in the NASA description fields?
```{r}
desc_tf_idf %>% 
  arrange(-tf_idf)
```

These are the most important words in the description fields as measured by `tf-idf`, meaning they are common but not too common.

> Notice we have run into an issue here; both `n` and `term frequency` are equal to 1 for these terms, meaning that these were description fields that only had a single word in them. If a description field only contains one word, the tf-idf algorithm will think that is a very important word.

Depending on our analytics goals, it might be a good idea to throw out all description fileds that have very few words.

### 8.3.2 Connecting description fields to keywords
We now know which words in the descriptions have high tf-idf, and we also have labels for these descriptions in the keywords. Let’s do a full join of the keyword data frame and the data frame of description words with tf-idf, and then find the highest tf-idf words for a given keyword.

```{r}
desc_tf_idf <- full_join(desc_tf_idf, nasa_keyword, by = "id")
```

Let's plot some of the most important words, as measured by tf-idf a few example keywords used on NASA datasets. First, let's use dplyr operations to filter for the keywords we want to examine and take just the top 15 words for each keyword. Then, let's plot those words in Figure 8.5.
```{r}
desc_tf_idf %>% 
  filter(!near(tf,1)) %>% 
  filter(keyword %in%)
```





